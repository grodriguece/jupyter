{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Custom callback \n",
    "\n",
    "This is a notebook for the medium article [A practical introduction to Keras Callbacks](https://medium.com/@bindiatwork/a-practical-introduction-to-keras-callbacks-in-tensorflow-2-705d0c584966)\n",
    "\n",
    "Please check out article for instructions\n",
    "\n",
    "**License**: [BSD 2-Clause](https://opensource.org/licenses/BSD-2-Clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics)\n",
    "    plt.plot(epochs, val_metrics)\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr(history):\n",
    "    learning_rate = history.history['lr']\n",
    "    epochs = range(1, len(learning_rate) + 1)\n",
    "    plt.plot(epochs, learning_rate)\n",
    "    plt.title('Learning rate')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MMIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster training, let's use a subset 10,000\n",
    "X_train, y_train = X_train_full[:10000] / 255.0, y_train_full[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "def create_model(): \n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(10, activation='softmax'),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='sgd', \n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a custom callback for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class TrainingCallback(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        print(f\"Training: Starting batch {batch}\")\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(f\"Training: Finished batch {batch}, loss is {logs['loss']}\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Finished epoch {epoch}, loss is {logs['loss']}, accuracy is {logs['accuracy']}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Starting training...\n",
      "Starting epoch 0\n",
      "Epoch 1/2\n",
      "Training: Starting batch 0\n",
      "Training: Finished batch 0, loss is 0.3087729215621948\n",
      "Training: Starting batch 1\n",
      "Training: Finished batch 1, loss is 0.302765429019928\n",
      "Finished epoch 0, loss is 0.3057691752910614, accuracy is 0.8997499942779541\n",
      "8000/8000 - 1s - loss: 0.3058 - accuracy: 0.8997 - val_loss: 0.4249 - val_accuracy: 0.8515\n",
      "Starting epoch 1\n",
      "Epoch 2/2\n",
      "Training: Starting batch 0\n",
      "Training: Finished batch 0, loss is 0.2967260777950287\n",
      "Training: Starting batch 1\n",
      "Training: Finished batch 1, loss is 0.31522929668426514\n",
      "Finished epoch 1, loss is 0.3059776872396469, accuracy is 0.8993750214576721\n",
      "8000/8000 - 0s - loss: 0.3060 - accuracy: 0.8994 - val_loss: 0.4249 - val_accuracy: 0.8510\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=2, \n",
    "    validation_split=0.20, \n",
    "    batch_size=4000, \n",
    "    verbose=2,\n",
    "    callbacks=[TrainingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a custom callback for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingCallback(Callback):\n",
    "    \n",
    "    def on_test_begin(self, logs=None):\n",
    "        print(\"Starting testing ...\")\n",
    "        \n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        print(f\"Testing: Starting batch {batch}\")\n",
    "    \n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(f\"Testing: Finished batch {batch}\")\n",
    "        \n",
    "    def on_test_end(self, logs=None):\n",
    "        print(\"Finished testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing ...\n",
      "Testing: Starting batch 0\n",
      "Testing: Finished batch 0\n",
      "Testing: Starting batch 1\n",
      "Testing: Finished batch 1\n",
      "Testing: Starting batch 2\n",
      "Testing: Finished batch 2\n",
      "Testing: Starting batch 3\n",
      "Testing: Finished batch 3\n",
      "Testing: Starting batch 4\n",
      "Testing: Finished batch 4\n",
      "Finished testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[85.61210479736329, 0.8063]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=False, callbacks=[TestingCallback()], batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a custom callback for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionCallback(Callback):\n",
    "    \n",
    "    def on_predict_begin(self, logs=None):\n",
    "        print(\"Starting prediction ...\")\n",
    "    \n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        print(f\"Prediction: Starting batch {batch}\")\n",
    "        \n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        print(f\"Prediction: Finish batch {batch}\")\n",
    "    \n",
    "    def on_predict_end(self, logs=None):\n",
    "        print(\"Finished prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction ...\n",
      "Prediction: Starting batch 0\n",
      "Prediction: Finish batch 0\n",
      "Prediction: Starting batch 1\n",
      "Prediction: Finish batch 1\n",
      "Prediction: Starting batch 2\n",
      "Prediction: Finish batch 2\n",
      "Prediction: Starting batch 3\n",
      "Prediction: Finish batch 3\n",
      "Prediction: Starting batch 4\n",
      "Prediction: Finish batch 4\n",
      "Finished prediction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00],\n",
       "       [0.000000e+00, 0.000000e+00, 1.000000e+00, ..., 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00],\n",
       "       [0.000000e+00, 1.000000e+00, 0.000000e+00, ..., 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00],\n",
       "       ...,\n",
       "       [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00],\n",
       "       [0.000000e+00, 1.000000e+00, 0.000000e+00, ..., 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00],\n",
       "       [0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 2.315278e-25,\n",
       "        0.000000e+00, 0.000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test, verbose=False, callbacks=[PredictionCallback()], batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examples of Keras callback applications\n",
    "Reference: https://keras.io/guides/writing_your_own_callbacks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Early stopping at minimum loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 - 2s - loss: 1.2644 - accuracy: 0.6643 - val_loss: 1.0006 - val_accuracy: 0.6845\n",
      "Epoch 2/50\n",
      "8000/8000 - 1s - loss: 0.8767 - accuracy: 0.7197 - val_loss: 0.8218 - val_accuracy: 0.7325\n",
      "Epoch 3/50\n",
      "8000/8000 - 1s - loss: 0.7529 - accuracy: 0.7579 - val_loss: 0.7451 - val_accuracy: 0.7475\n",
      "Epoch 4/50\n",
      "8000/8000 - 1s - loss: 0.6833 - accuracy: 0.7774 - val_loss: 0.6963 - val_accuracy: 0.7685\n",
      "Epoch 5/50\n",
      "8000/8000 - 1s - loss: 0.6363 - accuracy: 0.7878 - val_loss: 0.6644 - val_accuracy: 0.7730\n",
      "Epoch 6/50\n",
      "8000/8000 - 1s - loss: 0.6030 - accuracy: 0.7996 - val_loss: 0.6251 - val_accuracy: 0.7880\n",
      "Epoch 7/50\n",
      "8000/8000 - 1s - loss: 0.5751 - accuracy: 0.8099 - val_loss: 0.5956 - val_accuracy: 0.7980\n",
      "Epoch 8/50\n",
      "8000/8000 - 1s - loss: 0.5531 - accuracy: 0.8166 - val_loss: 0.5838 - val_accuracy: 0.8005\n",
      "Epoch 9/50\n",
      "8000/8000 - 1s - loss: 0.5355 - accuracy: 0.8204 - val_loss: 0.5615 - val_accuracy: 0.8065\n",
      "Epoch 10/50\n",
      "8000/8000 - 1s - loss: 0.5192 - accuracy: 0.8253 - val_loss: 0.5488 - val_accuracy: 0.8165\n",
      "Epoch 11/50\n",
      "8000/8000 - 1s - loss: 0.5060 - accuracy: 0.8282 - val_loss: 0.5387 - val_accuracy: 0.8165\n",
      "Epoch 12/50\n",
      "8000/8000 - 1s - loss: 0.4932 - accuracy: 0.8315 - val_loss: 0.5416 - val_accuracy: 0.8085\n",
      "Epoch 13/50\n",
      "8000/8000 - 1s - loss: 0.4853 - accuracy: 0.8336 - val_loss: 0.5327 - val_accuracy: 0.8150\n",
      "Epoch 14/50\n",
      "8000/8000 - 1s - loss: 0.4773 - accuracy: 0.8391 - val_loss: 0.5196 - val_accuracy: 0.8245\n",
      "Epoch 15/50\n",
      "8000/8000 - 1s - loss: 0.4666 - accuracy: 0.8397 - val_loss: 0.5048 - val_accuracy: 0.8315\n",
      "Epoch 16/50\n",
      "8000/8000 - 1s - loss: 0.4597 - accuracy: 0.8382 - val_loss: 0.5060 - val_accuracy: 0.8185\n",
      "Epoch 17/50\n",
      "8000/8000 - 1s - loss: 0.4507 - accuracy: 0.8490 - val_loss: 0.5220 - val_accuracy: 0.8140\n",
      "Epoch 18/50\n",
      "8000/8000 - 1s - loss: 0.4456 - accuracy: 0.8455 - val_loss: 0.5078 - val_accuracy: 0.8185\n",
      "Epoch 19/50\n",
      "8000/8000 - 1s - loss: 0.4408 - accuracy: 0.8464 - val_loss: 0.4849 - val_accuracy: 0.8300\n",
      "Epoch 20/50\n",
      "8000/8000 - 1s - loss: 0.4341 - accuracy: 0.8509 - val_loss: 0.4959 - val_accuracy: 0.8225\n",
      "Epoch 21/50\n",
      "8000/8000 - 1s - loss: 0.4289 - accuracy: 0.8503 - val_loss: 0.4855 - val_accuracy: 0.8305\n",
      "Epoch 22/50\n",
      "8000/8000 - 1s - loss: 0.4237 - accuracy: 0.8519 - val_loss: 0.4764 - val_accuracy: 0.8365\n",
      "Epoch 23/50\n",
      "8000/8000 - 1s - loss: 0.4208 - accuracy: 0.8544 - val_loss: 0.4805 - val_accuracy: 0.8310\n",
      "Epoch 24/50\n",
      "8000/8000 - 1s - loss: 0.4152 - accuracy: 0.8576 - val_loss: 0.4792 - val_accuracy: 0.8285\n",
      "Epoch 25/50\n",
      "8000/8000 - 1s - loss: 0.4070 - accuracy: 0.8596 - val_loss: 0.4759 - val_accuracy: 0.8330\n",
      "Epoch 26/50\n",
      "8000/8000 - 1s - loss: 0.4023 - accuracy: 0.8616 - val_loss: 0.4817 - val_accuracy: 0.8260\n",
      "Epoch 27/50\n",
      "8000/8000 - 1s - loss: 0.3986 - accuracy: 0.8650 - val_loss: 0.4750 - val_accuracy: 0.8290\n",
      "Epoch 28/50\n",
      "Restoring model weights from the end of the best epoch.\n",
      "8000/8000 - 1s - loss: 0.3990 - accuracy: 0.8618 - val_loss: 0.4631 - val_accuracy: 0.8380\n",
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    validation_split=0.20, \n",
    "    batch_size=64, \n",
    "    verbose=2,\n",
    "    callbacks=[EarlyStoppingAtMinLoss()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (3, 0.05),\n",
    "    (6, 0.01),\n",
    "    (9, 0.005),\n",
    "    (12, 0.001),\n",
    "]\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "\n",
      "Epoch 00000: Learning rate is 0.0010.\n",
      "Epoch 1/15\n",
      "8000/8000 - 1s - loss: 0.3998 - accuracy: 0.8665 - val_loss: 0.4640 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00001: Learning rate is 0.0010.\n",
      "Epoch 2/15\n",
      "8000/8000 - 1s - loss: 0.3993 - accuracy: 0.8651 - val_loss: 0.4638 - val_accuracy: 0.8335\n",
      "\n",
      "Epoch 00002: Learning rate is 0.0010.\n",
      "Epoch 3/15\n",
      "8000/8000 - 1s - loss: 0.3987 - accuracy: 0.8643 - val_loss: 0.4638 - val_accuracy: 0.8350\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0500.\n",
      "Epoch 4/15\n",
      "8000/8000 - 1s - loss: 0.4711 - accuracy: 0.8365 - val_loss: 0.4964 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0500.\n",
      "Epoch 5/15\n",
      "8000/8000 - 1s - loss: 0.4348 - accuracy: 0.8494 - val_loss: 0.5464 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00005: Learning rate is 0.0500.\n",
      "Epoch 6/15\n",
      "8000/8000 - 1s - loss: 0.4169 - accuracy: 0.8508 - val_loss: 0.4499 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00006: Learning rate is 0.0100.\n",
      "Epoch 7/15\n",
      "8000/8000 - 1s - loss: 0.3632 - accuracy: 0.8771 - val_loss: 0.4373 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00007: Learning rate is 0.0100.\n",
      "Epoch 8/15\n",
      "8000/8000 - 1s - loss: 0.3567 - accuracy: 0.8781 - val_loss: 0.4400 - val_accuracy: 0.8455\n",
      "\n",
      "Epoch 00008: Learning rate is 0.0100.\n",
      "Epoch 9/15\n",
      "8000/8000 - 1s - loss: 0.3524 - accuracy: 0.8808 - val_loss: 0.4405 - val_accuracy: 0.8455\n",
      "\n",
      "Epoch 00009: Learning rate is 0.0050.\n",
      "Epoch 10/15\n",
      "8000/8000 - 1s - loss: 0.3464 - accuracy: 0.8839 - val_loss: 0.4329 - val_accuracy: 0.8480\n",
      "\n",
      "Epoch 00010: Learning rate is 0.0050.\n",
      "Epoch 11/15\n",
      "8000/8000 - 1s - loss: 0.3439 - accuracy: 0.8832 - val_loss: 0.4329 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00011: Learning rate is 0.0050.\n",
      "Epoch 12/15\n",
      "8000/8000 - 1s - loss: 0.3425 - accuracy: 0.8854 - val_loss: 0.4294 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00012: Learning rate is 0.0010.\n",
      "Epoch 13/15\n",
      "8000/8000 - 1s - loss: 0.3384 - accuracy: 0.8864 - val_loss: 0.4284 - val_accuracy: 0.8490\n",
      "\n",
      "Epoch 00013: Learning rate is 0.0010.\n",
      "Epoch 14/15\n",
      "8000/8000 - 1s - loss: 0.3380 - accuracy: 0.8866 - val_loss: 0.4293 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00014: Learning rate is 0.0010.\n",
      "Epoch 15/15\n",
      "8000/8000 - 1s - loss: 0.3376 - accuracy: 0.8869 - val_loss: 0.4292 - val_accuracy: 0.8520\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=15, \n",
    "    validation_split=0.20, \n",
    "    batch_size=64, \n",
    "    verbose=2,\n",
    "    callbacks=[CustomLearningRateScheduler(lr_schedule)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-tutorial",
   "language": "python",
   "name": "tf-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
